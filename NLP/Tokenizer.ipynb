{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f879434",
   "metadata": {},
   "source": [
    "- Transformer를 활용한 챗봇에서는 SubwordTextEncoder를 사용했습니다.\n",
    "- SubwordTextEncoder는 사전에 토큰을 추가하거나 업데이트하는 메서드가 없어 모델 입출력시 매번 START Token과 END Token을 추가해 주는것이 번거로워 SentencePiece로 Tokenizer를 변경하고자 합니다.\n",
    "- 그 과정에서 Tokenizer에 관한 내용도 한번 더 정리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88705bd",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    " - 입력된 문장들을 토큰이라는 작은 unit들로 나누어주는 역할을 한다.\n",
    " - 토큰들 숫자데이터로 변환해 NLP task에 사용한다.\n",
    " - 문자기반, 단어기반, 서브워드 토크나이저 등등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e49843",
   "metadata": {},
   "source": [
    "### 1. Word Tokenizer\n",
    " - 단어를 기준으로 토큰화를 하는 토크나이저\n",
    " - ex) \"맑은 하늘에 햇빛이 비쳐요.\" => '맑은', '하늘에', '햇빛이', '비쳐요', '.'\n",
    " - 단점으로, 어미 변화로 인해 유사단어가 많아질 수 있고 단어를 표현하는 벡터가 다른 의미를 가질 수 있다.\n",
    " - '하늘에', '하늘을', 하늘에서', '하늘이' 등\n",
    " - 사전의 개수가 매우 많이질 수 있다.\n",
    " \n",
    "### 2. Character Tokenizer\n",
    " - 개별 문자를 기준으로 토큰화를 하는 토크나이저\n",
    " - ex) \"안녕하세요\" => '안', '녕', '하', '세', '요'\n",
    " - OOV 발생 가능성이 낮다.\n",
    " - 단점으로 각 글자 '안', '녕', '하', '세', '요'의 벡터가 '안녕하세요'를 표현하지 못할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38871c1",
   "metadata": {},
   "source": [
    "### 3. Subword Tokenizer\n",
    " - 단어를 나누어 단어안에 단어(subword)들로 토큰화하는 토크나이저\n",
    " - 빈번하게 사용되는 단어는 분할하지 않고 드물게 등장하는 단어를 의미있는 하위 단어로 분할한다.\n",
    " - ex) '경찰차', '경찰청', '경찰관' => '경찰', '#차', '#청', '#관'\n",
    " - OOV 문제를 완화\n",
    " - BPE, SentencePiece, WordPiece 등 여러 방법이 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671fe4fb",
   "metadata": {},
   "source": [
    "#### 3-1) BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b426fa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T13:57:35.773796Z",
     "start_time": "2023-02-06T13:57:35.763958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 's')\n",
      "('es', 't')\n",
      "('est', '</w>')\n",
      "('l', 'o')\n",
      "('lo', 'w')\n",
      "('n', 'e')\n",
      "('ne', 'w')\n",
      "('new', 'est</w>')\n",
      "('low', '</w>')\n",
      "('w', 'i')\n"
     ]
    }
   ],
   "source": [
    "# BPE \n",
    "# byte pair encoding [](https://aclanthology.org/P16-1162/)\n",
    "# Neural Machine Translation of Rare Words with Subword Units\n",
    "# https://arxiv.org/abs/1508.07909\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "vocab = {\n",
    "    'l o w </w>' : 5,\n",
    "    'l o w e r </w>' : 2,\n",
    "    'n e w e s t </w>':6,\n",
    "    'w i d e s t </w>':3\n",
    "}\n",
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e904b3d",
   "metadata": {},
   "source": [
    "#### 3-2) SentencePiece\n",
    " - Subword Tokenizer를 모아둔 툴\n",
    " - GitHub : [SentencePiece Docs](https://github.com/google/sentencepiece/blob/master/python/README.md)\n",
    " - 몇가지 참고사항들 :[(.vocab file은 어떻게 사용?)](https://github.com/google/sentencepiece/issues/328), [(사용법에 관한 자세한 example)](https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb76936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T14:05:59.154769Z",
     "start_time": "2023-02-06T14:05:54.352768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\jaehy\\anaconda3\\lib\\site-packages (0.1.97)\n"
     ]
    }
   ],
   "source": [
    "# Install SentencePiece\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b422490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T09:24:35.782669Z",
     "start_time": "2023-02-07T09:24:34.501375Z"
    }
   },
   "outputs": [],
   "source": [
    "# SentencePiece\n",
    "import sentencepiece as spm\n",
    "\n",
    "corpus = \"data/chatbot_data.txt\"\n",
    "prefix = \"CBot_vocab\"\n",
    "vocab_size = 8000\n",
    "\n",
    "# train method creates 'm.model' and 'm.vocab'.\n",
    "# 'm.vocab' is just a reference. mainly use 'm.model'\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" +\n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" +    # maximum sentence length\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" +    # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" +    # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" +    # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" +    # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\")    # user defined custom token\n",
    "\n",
    "# More detail in Docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46404a61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T09:38:04.241641Z",
     "start_time": "2023-02-07T09:38:04.226682Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁저는', '▁금', '요일', '에', '▁친구와', '▁영화', '를', '▁볼', '▁거예요', '.']\n",
      "[615, 784, 2069, 6945, 5179, 697, 6982, 652, 24, 7564]\n",
      "\n",
      "저는 금요일에 친구와 영화를 볼 거예요.\n",
      "저는 금요일에 친구와 영화를 볼 거예요.\n"
     ]
    }
   ],
   "source": [
    "# Load the model file\n",
    "sp = spm.SentencePieceProcessor(model_file='CBot_vocab.model')\n",
    "\n",
    "# Encode: text => id\n",
    "sentences = [\n",
    "    \"저는 금요일에 친구와 영화를 볼 거예요.\",\n",
    "    \"안녕하세요, 오늘은 날씨가 좋습니다.\"]\n",
    "print(sp.encode_as_pieces(sentences[0]))\n",
    "print(sp.encode_as_ids(sentences[0]))\n",
    "print()\n",
    "\n",
    "# Decode: id => text\n",
    "print(sp.decode_pieces(['▁저는', '▁금', '요일', '에', '▁친구와', '▁영화', '를', '▁볼', '▁거예요', '.']))\n",
    "print(sp.decode_ids([615, 784, 2069, 6945, 5179, 697, 6982, 652, 24, 7564]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c6252b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T09:40:32.992227Z",
     "start_time": "2023-02-07T09:40:32.987228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8007\n",
      "1\n",
      "[UNK]\n"
     ]
    }
   ],
   "source": [
    "# Vocab size\n",
    "print(sp.vocab_size())\n",
    "\n",
    "# UNK Token return 1\n",
    "print(sp.piece_to_id('__언노운토큰'))\n",
    "print(sp.id_to_piece(1))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
