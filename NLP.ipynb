{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "64QqelojKklR",
        "VmfTzKJ1wM1L",
        "vHLX9ryfVc-n",
        "mho958P8QkHN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 자연어 처리 (NLP, Natural Language Processing)\n",
        "\n",
        "- **`자연어`**\n",
        "  - 사람들이 일상적으로 사용하는 언어 (↔ 인공어)\n",
        "  - 음성, 텍스트\n",
        "  - 불규칙적\n",
        "\n",
        "- **`자연어 처리`**\n",
        "  - 자연어를 기계를 이용해 모사할 수 있도록 연구하고 이를 구현하는 AI분야\n",
        "\n",
        "<!-- - **`처리 과정`**\n",
        "  - 이해과정 (NLUnderstanding) : 화자의 의도 파악\n",
        "  - 문장생성과정 (NLGeneration) : 언어로 표현 -->\n",
        "\n",
        "- **`단어 표현`**: `텍스트`를 `숫자`로 바꿔야함 (기계가 인식할 수 있도록)\n",
        "  1. 주어진 텍스트 데이터를 문장이나 단어 단위로 토크나이징\n",
        "  2. 불용어 제거 후 vocabulary(사전) 생성\n",
        "  3. stemming이나 lemmatization으로 특징 추출\n",
        "  4. 3을 TF-IDF같은 툴로 특징 추출한 결과물을 모델에 이용\n"
      ],
      "metadata": {
        "id": "pmEgdYWJ5XNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **sklearn을 이용한 특징 추출**\n",
        "  - 특징 추출: 텍스트 데이터에서 단어나 문장들을 어떤 특정 값으로 바꿔주는 것\n",
        "    - `CountVectorizer`: **횟수**를 기준으로 특징 추출\n",
        "    - `TfidfVectorizer`: **TF**와 **IDF**를 이용해 특징 추출 (TF가 높을수록, DF가 낮을수록 중요한 단어)\n",
        "    - `HashingVectorizer`"
      ],
      "metadata": {
        "id": "64QqelojKklR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "WZ_PhwX85QUO"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "\n",
        "text_data = ['나는 배가 고프다', '내일 점심 뭐먹지', '내일 공부 해야겠다', '점심 먹고 공부 해야지']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer\n",
        "count_vectorizer = CountVectorizer()    # CountVectorizer 객체 생성\n",
        "count_vectorizer.fit(text_data)         # fitting\n",
        "print('cnt vec:', count_vectorizer.vocabulary_)     # vocabulary 확인\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(text_data)\n",
        "print('tfidf:', tfidf_vectorizer.vocabulary_)\n",
        "\n",
        "# Hashing\n",
        "hash_vectorizer = HashingVectorizer(n_features=10)    # n_features: 사전의 크기에 해당\n",
        "hash_vectorizer.fit(text_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCv9n2dLfe1",
        "outputId": "77bd56a2-a5f1-4db8-87b0-a6ec8b301c6f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cnt vec: {'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n",
            "tfidf: {'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HashingVectorizer(n_features=10)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# *** 아래 두 dictionary 만들어서 사용 ***\n",
        "word2idx = count_vectorizer.vocabulary_             \n",
        "idx2word = {v:k for (k, v) in word2idx.items()}     # 역으로 value값으로 key를 찾을 수 있도록 바꾼 dictionary를 하나 만들어줌\n",
        "print(word2idx)\n",
        "print(idx2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KF9iZ7QMLjm",
        "outputId": "a9e6dc11-aa50-45f8-88be-5191a94c44a2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n",
            "{2: '나는', 6: '배가', 0: '고프다', 3: '내일', 7: '점심', 5: '뭐먹지', 1: '공부', 8: '해야겠다', 4: '먹고', 9: '해야지'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = [text_data[0]]\n",
        "\n",
        "# CountVectorizer\n",
        "print(count_vectorizer.transform(sentence).toarray())    # 단점: 실제 문장 순서와 다름\n",
        "count_vectorizer.transform(sentence)                     # 1인 위치만 기록하고 나머지는 0으로 저장하는 방식\n",
        "\n",
        "# TF-IDF\n",
        "print()\n",
        "print(tfidf_vectorizer.transform(sentence).toarray())\n",
        "\n",
        "# Hashing\n",
        "print()\n",
        "print(hash_vectorizer.transform(sentence).toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEUuib_FLmVM",
        "outputId": "028c1d7e-e549-4d9e-8166-e51b6c386a1c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 1 0 0 0 1 0 0 0]]\n",
            "\n",
            "[[0.57735027 0.         0.57735027 0.         0.         0.\n",
            "  0.57735027 0.         0.         0.        ]]\n",
            "\n",
            "[[ 0.          0.57735027  0.          0.          0.57735027  0.\n",
            "  -0.57735027  0.          0.          0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **자연어 토크나이징**\n",
        "  - 텍스트에 대한 정보를 단어나 문장같은 **특정 단위**별로 나눔\n",
        "  \n"
      ],
      "metadata": {
        "id": "VmfTzKJ1wM1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 영어 토크나이징"
      ],
      "metadata": {
        "id": "vHLX9ryfVc-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')        # \n",
        "nltk.download('stopwords')    # 불용어 목록"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBvroZs-L2Cc",
        "outputId": "71789372-1a96-4a0c-ac6d-c28a8f202c91"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "paragraph = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of computer science, information engineering, \n",
        "and artificial intelligence concerned with the interactions between computers and human (natural) languages, \n",
        "in particular how to program computers to process and analyze large amounts of natural language data. \n",
        "Challenges in natural language processing frequently involve speech recognition, natural language understanding, \n",
        "and natural language generation.\n",
        "\"\"\"\n",
        "sent_tokens = sent_tokenize(paragraph)\n",
        "print(sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_rZm4xOxLEN",
        "outputId": "4fa1bffd-dd62-4ca6-eb8c-90afa5ac2614"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\nNatural language processing (NLP) is a subfield of computer science, information engineering, \\nand artificial intelligence concerned with the interactions between computers and human (natural) languages, \\nin particular how to program computers to process and analyze large amounts of natural language data.', 'Challenges in natural language processing frequently involve speech recognition, natural language understanding, \\nand natural language generation.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = [word_tokenize(sent_token) for sent_token in sent_tokens]\n",
        "word_tokens[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YzK358GxmJ-",
        "outputId": "bb28331d-7e3e-49c5-93c2-0b9f024d4cbf"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Challenges',\n",
              " 'in',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'frequently',\n",
              " 'involve',\n",
              " 'speech',\n",
              " 'recognition',\n",
              " ',',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'understanding',\n",
              " ',',\n",
              " 'and',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'generation',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop word\n",
        "stopwords = nltk.corpus.stopwords.words('english')    # 등록된 stop word\n",
        "stopwords.append(',')   # 원하는 불용어 추가\n",
        "\n",
        "stopwords[:15]  # 불용어 목록 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKw7SVDVyMnk",
        "outputId": "298c0e9e-22f0-4a52-d77e-9f513d4fa571"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어 제거\n",
        "all_tokens = []\n",
        "for sent in word_tokens:\n",
        "  all_tokens.append([word for word in sent if word.lower() not in stopwords])\n",
        "all_tokens[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxX9iRLyy1-h",
        "outputId": "2a0d89d1-bc27-4a1d-d872-a90b9e4e979f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Challenges',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'frequently',\n",
              " 'involve',\n",
              " 'speech',\n",
              " 'recognition',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'understanding',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'generation',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming, 어간 추출 (단어에서 변하지 않는 부분)\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "for word in ['working', 'works', 'worked']:\n",
        "  print(stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrvZhLpdzjFq",
        "outputId": "f1dd8277-fcf4-4a39-e5cf-33fc9e214ed1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work\n",
            "work\n",
            "work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization (표제어 추출)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "for word in ['working', 'works', 'worked']:\n",
        "  print(lemma.lemmatize(word, 'v'))   # v: 동사\n",
        "\n",
        "for word in ['happier', 'happiest']:\n",
        "  print(lemma.lemmatize(word, 'a'))   # a: 형용사"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Se6LUCH0ufP",
        "outputId": "6b7c5d15-be39-4111-ecba-952d0b20848a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "work\n",
            "work\n",
            "work\n",
            "happy\n",
            "happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in all_tokens[1]:\n",
        "  print(stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKoOdDkH2sQZ",
        "outputId": "b8ee4906-ddb0-4a48-8dd0-40dd54d86192"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "challeng\n",
            "nat\n",
            "langu\n",
            "process\n",
            "frequ\n",
            "involv\n",
            "speech\n",
            "recognit\n",
            "nat\n",
            "langu\n",
            "understand\n",
            "nat\n",
            "langu\n",
            "gen\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장이나 단어 단위로 토크나이징(word.tokens) -> 불용어제거하고 volcabulary 생성(all_tokens) -> stemming -> TF-IDF\n",
        "word2idx = {}\n",
        "n_idx = 0\n",
        "for sent in all_tokens:\n",
        "  for word in sent:\n",
        "    if word.lower() not in word2idx:\n",
        "      word2idx[word.lower()] = n_idx\n",
        "      n_idx += 1\n",
        "\n",
        "idx2word = {v:k for k, v in word2idx.items()}"
      ],
      "metadata": {
        "id": "2cVhQeCu3hDx"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx"
      ],
      "metadata": {
        "id": "6cQfKGlV-dP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "senten = []\n",
        "for sent in all_tokens:\n",
        "  senten.append([word2idx[word.lower()] for word in sent])\n",
        "print(senten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V6m4EaS-eGo",
        "outputId": "913c4a25-b2ef-449a-91ca-33ad69c6bded"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 3, 0, 5, 17, 18, 19, 15, 20, 21, 22, 23, 0, 1, 24, 25], [26, 0, 1, 2, 27, 28, 29, 30, 0, 1, 31, 0, 1, 32, 25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### POS tag, part of Speech(품사) tagging\n"
      ],
      "metadata": {
        "id": "WWJzTSENM1Es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tag\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')   # 품사 태깅을 위해 필요한 도구"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLmt2qIlArJo",
        "outputId": "923cf69f-e098-4364-e2e1-5b528cda336c"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of computer science, information engineering, \n",
        "and artificial intelligence concerned with the interactions between computers and human (natural) languages, \n",
        "in particular how to program computers to process and analyze large amounts of natural language data. \n",
        "\"\"\"\n",
        "word_tokens = word_tokenize(sentence)\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuuEw4rcLxfR",
        "outputId": "5394be56-8254-425a-f1cf-8cb13c87a693"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'computer', 'science', ',', 'information', 'engineering', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(word_tokens)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB5yX4L7NF5i",
        "outputId": "e9c8e961-cd4f-4a2e-8d4d-624e106d75cd"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Natural', 'JJ'),\n",
              " ('language', 'NN'),\n",
              " ('processing', 'NN'),\n",
              " ('(', '('),\n",
              " ('NLP', 'NNP')]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 명사(NN)와 형용사(JJ)만 표시\n",
        "sent_nnjj = [word for word, pos in nltk.pos_tag(word_tokens) if pos == 'NN' or pos == 'JJ']\n",
        "sent_nnjj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRuiF-4iNIHK",
        "outputId": "e51734cb-6f43-4dc1-fbc5-a58cd9cd8100"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'subfield',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'information',\n",
              " 'engineering',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'human',\n",
              " 'natural',\n",
              " 'particular',\n",
              " 'program',\n",
              " 'large',\n",
              " 'natural',\n",
              " 'language']"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram, trigram, n-gram(n개 묶음)\n",
        "bigram = [(a, b) for a, b in nltk.bigrams(sent_nnjj)]\n",
        "trigram = [(a, b, c) for a, b, c in nltk.trigrams(sent_nnjj)]\n",
        "ngram = [(a, b, c, d) for a, b, c, d in nltk.ngrams(sent_nnjj, 4)]\n",
        "\n",
        "print(bigram, trigram, ngram)"
      ],
      "metadata": {
        "id": "qTIUQgiNOG0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 한글 토크나이징"
      ],
      "metadata": {
        "id": "mho958P8QkHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n",
        "\n",
        "import konlpy\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zKQ0QpuO2Qu",
        "outputId": "12079b23-a37f-4299-b455-b366273d7685"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 36.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '한글 자연어 처리는 재밌다 이제부터 열심히 해야지 ㅎㅎㅎ.'\n",
        "\n",
        "print(okt.morphs(text))\n",
        "print(okt.morphs(text, stem=True))    # 형태소 단위로 나눈 후 어간 추출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB5R05ejTSbp",
        "outputId": "65dc0d5e-e1b7-466a-c24d-62e594d235f9"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['한글', '자연어', '처리', '는', '재밌다', '이제', '부터', '열심히', '해야지', 'ㅎㅎㅎ', '.']\n",
            "['한글', '자연어', '처리', '는', '재밌다', '이제', '부터', '열심히', '하다', 'ㅎㅎㅎ', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 명사 추출\n",
        "print('명사추출:', okt.nouns(text))\n",
        "\n",
        "# 어절 추출\n",
        "print('어절 추출:', okt.phrases(text))\n",
        "\n",
        "# 품사 태깅\n",
        "print('품사 태깅')\n",
        "print(okt.pos(text))\n",
        "print(okt.pos(text, join=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwqS74IYTcOg",
        "outputId": "fe63ffa4-dc89-4bbf-9dfd-73b011c39399"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "명사추출: ['한글', '자연어', '처리', '이제']\n",
            "어절 추출: ['한글', '한글 자연어', '한글 자연어 처리', '이제', '자연어', '처리']\n",
            "품사 태깅\n",
            "[('한글', 'Noun'), ('자연어', 'Noun'), ('처리', 'Noun'), ('는', 'Josa'), ('재밌다', 'Adjective'), ('이제', 'Noun'), ('부터', 'Josa'), ('열심히', 'Adverb'), ('해야지', 'Verb'), ('ㅎㅎㅎ', 'KoreanParticle'), ('.', 'Punctuation')]\n",
            "['한글/Noun', '자연어/Noun', '처리/Noun', '는/Josa', '재밌다/Adjective', '이제/Noun', '부터/Josa', '열심히/Adverb', '해야지/Verb', 'ㅎㅎㅎ/KoreanParticle', './Punctuation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tRgu2-_QTtnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ztx-eI4-e9at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 단어 표현 (Word Representation)\n",
        "  - word embedding, word vector라고도 불림\n",
        "  - `분포가설` (Distributed hypothesis)\n",
        "    - 같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다\n",
        "  - `카운트 기반 방법`과 `예측 기반 방법`\n",
        "   1. `카운트 기반 방법` : 특정 문맥 안에서 단어들이 동시에 등장하는 횟수를 세는 방법\n",
        "      - **동시발생행렬**(Co-occurence Matrix) 생성 후 행렬을 수치화해서 단어벡터로 만드는 방법을 사용하는 방식\n",
        "        - 특이값 분해 (SVD)\n",
        "        - 잠재의미분석 (Latent Semantic Analysis, LSA)\n",
        "        - Hyperspace Analogue to Language (HAL)\n",
        "        - Hellinger PCA (Principal Component Analysis)\n",
        "   2. `학습 기반 방법` : 신경망 등을 통해 문맥 안의 단어들을 예측하는 방법\n",
        "    - Word2vec\n"
      ],
      "metadata": {
        "id": "dOgidX95jTr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Co-occurence matrix 생성 방법"
      ],
      "metadata": {
        "id": "feJ59vDwpeTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 카운트 기반 방법에 사용되는 동시발생행렬 생성\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "text = ['성진과 창욱은 야구장에 갔다',\n",
        "        '성진과 태균은 도서관에 갔다',\n",
        "        '성진과 창욱은 공부를 좋아한다']"
      ],
      "metadata": {
        "id": "9PgDSYkpe_Bl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer()\n",
        "x = cv.fit_transform(text).toarray()\n",
        "print(x)\n",
        "print(x.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ-5sHJBfAwN",
        "outputId": "ab267735-8fdb-40d0-8ee8-d7e2878c8dd0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 1 1 0 1 0]\n",
            " [1 0 1 1 0 0 0 1]\n",
            " [0 1 0 1 0 1 1 0]]\n",
            "[[1 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 1 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 1]\n",
            " [0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C = np.dot(x.T, x)    # 동시발생행렬\n",
        "np.fill_diagonal(C, 0)\n",
        "C"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plx3ktGVf3OD",
        "outputId": "5bdd45a1-c707-4723-9b17-38a29a698408"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 2, 1, 0, 1, 1],\n",
              "       [0, 0, 0, 1, 0, 1, 1, 0],\n",
              "       [1, 0, 0, 1, 0, 0, 0, 1],\n",
              "       [2, 1, 1, 0, 1, 1, 2, 1],\n",
              "       [1, 0, 0, 1, 0, 0, 1, 0],\n",
              "       [0, 1, 0, 1, 0, 0, 1, 0],\n",
              "       [1, 1, 0, 2, 1, 1, 0, 0],\n",
              "       [1, 0, 1, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 학습 기반 방법"
      ],
      "metadata": {
        "id": "VF9yMwKRphgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PHSGfzAXpi34"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}